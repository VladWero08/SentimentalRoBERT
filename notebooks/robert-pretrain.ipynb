{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:19:48.832285Z","iopub.status.busy":"2024-05-14T16:19:48.831441Z","iopub.status.idle":"2024-05-14T16:19:58.626945Z","shell.execute_reply":"2024-05-14T16:19:58.625807Z","shell.execute_reply.started":"2024-05-14T16:19:48.832239Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc1ab57aadde4793bb399e26909448a8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"805e51cf43ce4a1ab5453399f50d2960","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/245k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcad3809dff7414c8fe5cf795cb49d38","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbbff3afabb74f51aa4f32ef9a9ad526","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9db454eb041a4464b752de2d47f7383c","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","\n","tokenizer = BertTokenizer.from_pretrained(\"readerbench/RoBERT-small\")\n","model_emotional = BertForMaskedLM.from_pretrained(\"readerbench/RoBERT-small\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:20:05.836969Z","iopub.status.busy":"2024-05-14T16:20:05.836442Z","iopub.status.idle":"2024-05-14T16:20:06.397971Z","shell.execute_reply":"2024-05-14T16:20:06.396827Z","shell.execute_reply.started":"2024-05-14T16:20:05.836940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Indexes of the IDs that will be transformed into [MASK]: [1, 15, 28, 32, 39, 41, 46, 52, 71, 81, 88, 98, 102, 104, 105, 114, 115, 122, 134, 140, 145]\n","tensor(16.3362, grad_fn=<NllLossBackward0>)\n"]}],"source":["text = \"\"\"Intr-o dimineata senina, cand soarele inca se ridica deasupra orizontului, o pasare cantatoare si-a inceput \n","trilurile sale melodioase. In mijlocul unei paduri dense, animalele isi incepeau ziua in liniste. Un vant usor adia \n","printre crengile copacilor, iar frunzele se miscau in dansul lor natural. In adancul padurii, un rau curgea lin, izvorand viata \n","in jurul sau. Pe malul acestuia, o familie de iepuri se juca in iarba proaspata, fara griji sau framantari. Totul parea perfect \n","in armonia sa simpla si pura. Dar, in spatele acestei scene idilice, se ascundeau si intamplari mai putin linistite.\"\"\"\n","\n","inputs = tokenizer.encode_plus(\n","    text,\n","    add_special_tokens=True,\n","    max_length=512,\n","    padding=\"max_length\",\n","    truncation=True,\n","    return_token_type_ids=False,\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"\n",")\n","inputs[\"labels\"] = inputs.input_ids.detach().clone()\n","\n","cls_token_id = 3\n","sep_token_id = 4\n","mask_token_id = 5\n","\n","# generate a random number for each token to be associated with\n","masked_ids = torch.rand(inputs.input_ids.shape)\n","# the tokens that will be masked should have their random number < 0.15\n","# and should not be [CLS] or [SEP] tokens\n","masked_ids = (masked_ids < 0.15) * (inputs.input_ids != cls_token_id) * (inputs.input_ids != sep_token_id) * (inputs.input_ids != 0)\n","masked_ids = masked_ids.squeeze()\n","# get the actual ids that need to be transformed into [MASK] tokens\n","masked_ids = [index for index, is_maskable in enumerate(masked_ids) if is_maskable.item() is True]\n","\n","inputs.input_ids[0, masked_ids] = mask_token_id\n","\n","outputs = model_emotional(**inputs)\n","\n","print(f\"Indexes of the IDs that will be transformed into [MASK]: {masked_ids}\")\n","print(outputs.loss)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:14:36.566873Z","iopub.status.busy":"2024-05-14T17:14:36.566336Z","iopub.status.idle":"2024-05-14T17:14:36.580537Z","shell.execute_reply":"2024-05-14T17:14:36.579209Z","shell.execute_reply.started":"2024-05-14T17:14:36.566826Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class RomanianCorpusDataset(Dataset):\n","    # ids for the special tokens, should be checked \n","    # if they match with the tokenizer used\n","    cls_token_id = 3\n","    sep_token_id = 4\n","    mask_token_id = 5\n","    \n","    def __init__(self, texts: list, max_length: int = 512):\n","        # encode all the text from the texts list\n","        self.max_length = max_length\n","        self.inputs = [self.encode(text) for text in texts]\n","    \n","    def encode(self, text: str):\n","        # for training the MLM model, we only need: \n","        # 'input_ids', 'token_type_ids', 'attention_mask' and 'labels'\n","        encoded_text = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_token_type_ids=False,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\"\n","        )\n","        \n","        # set the labels to be equal to the original input_ids,\n","        # generated from the text that does not contain [MASK] tokens\n","        encoded_text[\"labels\"] = encoded_text.input_ids.detach().clone()\n","        \n","        # get masked ids for the current encoded text\n","        masked_ids = self.get_masked_ids(encoded_text)\n","        \n","        # transform the tokens with the previously generated IDs\n","        # into [MASK] tokens\n","        encoded_text.input_ids[0, masked_ids] = self.mask_token_id\n","        \n","        return encoded_text\n","    \n","    def get_masked_ids(self, encoded_text):\n","        # generate a random number for each token to be associated with\n","        masked_ids = torch.rand(encoded_text.input_ids.shape)\n","        # the tokens that will be masked should have their random number < 0.15\n","        # and should not be [CLS], [SEP] or empty tokens,\n","        masked_ids = (masked_ids < 0.15) * (encoded_text.input_ids != self.cls_token_id) * (encoded_text.input_ids != self.sep_token_id) * (inputs.input_ids != 0)\n","        masked_ids = masked_ids.squeeze()\n","        # get the actual ids that need to be transformed into [MASK] tokens\n","        masked_ids = [index for index, is_maskable in enumerate(masked_ids) if is_maskable.item() is True]\n","        \n","        return masked_ids\n","    \n","    def __len__(self):\n","        return len(self.inputs)\n","    \n","    def __getitem__(self, index):\n","        return {**self.inputs[index]}\n","        "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:14:39.526007Z","iopub.status.busy":"2024-05-14T17:14:39.525128Z","iopub.status.idle":"2024-05-14T17:14:39.558447Z","shell.execute_reply":"2024-05-14T17:14:39.557277Z","shell.execute_reply.started":"2024-05-14T17:14:39.525970Z"},"trusted":true},"outputs":[],"source":["# load the romanian emotional dataset, that contains\n","# different texts from fiction\n","import pandas as pd\n","\n","emotional_dataframe = pd.read_json(\"/kaggle/input/romanianfictionforbert/emotional.json\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:14:41.835577Z","iopub.status.busy":"2024-05-14T17:14:41.834899Z","iopub.status.idle":"2024-05-14T17:14:41.886402Z","shell.execute_reply":"2024-05-14T17:14:41.885334Z","shell.execute_reply.started":"2024-05-14T17:14:41.835545Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of texts extracted, of size 100: 2979 texts\n"]}],"source":["max_emotional_texts = 3000\n","max_emotional_text_size = 100\n","emotional_texts = []\n","\n","for text in emotional_dataframe.content:\n","    # stop condition for the whole extraction process, \n","    # because it does not need to extract more texts \n","    # if the maximum number of texts its reached\n","    if len(emotional_texts) >= max_emotional_texts:\n","        break\n","    \n","    splitted_text = text.split()\n","    \n","    for index in range(0, len(splitted_text), max_emotional_text_size):\n","        # stop condition for the whole extraction process, \n","        # because it does not need to extract more texts \n","        # if the maximum number of texts its reached\n","        if len(emotional_texts) >= max_emotional_texts:\n","            break\n","        \n","        # the last text might contain less words, \n","        # so it is not needed\n","        if len(splitted_text[index:index + max_emotional_text_size]) < max_emotional_text_size:\n","            break\n","        \n","        mini_text = \" \".join(splitted_text[index:index + max_emotional_text_size])  \n","        emotional_texts.append(mini_text)\n","        \n","print(f\"Number of texts extracted, of size {max_emotional_text_size}: {len(emotional_texts)} texts\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:14:49.582914Z","iopub.status.busy":"2024-05-14T17:14:49.582532Z","iopub.status.idle":"2024-05-14T17:15:06.359907Z","shell.execute_reply":"2024-05-14T17:15:06.359045Z","shell.execute_reply.started":"2024-05-14T17:14:49.582887Z"},"trusted":true},"outputs":[],"source":["batch_size = 5\n","\n","train_dataset = RomanianCorpusDataset(emotional_texts)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:15:09.663090Z","iopub.status.busy":"2024-05-14T17:15:09.662299Z","iopub.status.idle":"2024-05-14T17:15:09.681894Z","shell.execute_reply":"2024-05-14T17:15:09.680814Z","shell.execute_reply.started":"2024-05-14T17:15:09.663050Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device used for training and testing: cuda\n"]}],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","# set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device used for training and testing: {device.type}\")\n","\n","# move the MLM model to the device\n","model_emotional = model_emotional.to(device)\n","\n","# set the optimizer\n","optimizer = AdamW(model_emotional.parameters(), lr=1e-4)\n","\n","num_epochs = 4\n","\n","# total number of training steps\n","num_train_steps = len(train_dataloader) * num_epochs\n","\n","# scheduler (optional, for learning rate decay)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_train_steps\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:15:12.948813Z","iopub.status.busy":"2024-05-14T17:15:12.948404Z","iopub.status.idle":"2024-05-14T17:15:12.957915Z","shell.execute_reply":"2024-05-14T17:15:12.956802Z","shell.execute_reply.started":"2024-05-14T17:15:12.948782Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train(\n","    model: BertForMaskedLM,\n","    dataloader: DataLoader,\n","    optimizer,\n","    device: torch.device,\n","    scheduler=None\n",") -> float:\n","    \"\"\"\n","    ## Returns: epoch_loss\n","    - `epoch_loss`: float = overall loss for the epoch executed\n","    \"\"\"\n","    model.train()\n","    epoch_loss = 0.0\n","    \n","    for index, batch in tqdm(enumerate(dataloader)):\n","        input_ids = batch[\"input_ids\"].squeeze(1).to(device) \n","        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n","        labels = batch[\"labels\"].squeeze(1).to(device)\n","        \n","        # reset gradients for model's parameters\n","        model.zero_grad()\n","        \n","        # feed forward\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        \n","        # compute gradients\n","        loss = outputs.loss\n","        loss.backward()\n","        \n","        # optimize model's parameters\n","        optimizer.step()\n","    \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(dataloader)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:15:25.150503Z","iopub.status.busy":"2024-05-14T17:15:25.149884Z","iopub.status.idle":"2024-05-14T17:28:40.098047Z","shell.execute_reply":"2024-05-14T17:28:40.096968Z","shell.execute_reply.started":"2024-05-14T17:15:25.150474Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  6.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.20422787600565676\n","\n","Epoch 2/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  6.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.16628909411046328\n","\n","Epoch 3/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  5.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14190731558873748\n","\n","Epoch 4/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  5.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.1202056413678915\n","\n","Epoch 5/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  5.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.10051292147262385\n","\n","Epoch 6/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  6.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.08232778170764846\n","\n","Epoch 7/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  6.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.06666042410062263\n","\n","Epoch 8/8\n"]},{"name":"stderr","output_type":"stream","text":["596it [01:39,  5.99it/s]"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.053354949552890836\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["num_epochs = 8\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    loss = train(model_emotional, train_dataloader, optimizer, device, scheduler)\n","    print(f\"Loss: {loss}\")\n","    print()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:30:50.485380Z","iopub.status.busy":"2024-05-14T17:30:50.484978Z","iopub.status.idle":"2024-05-14T17:30:50.701718Z","shell.execute_reply":"2024-05-14T17:30:50.700554Z","shell.execute_reply.started":"2024-05-14T17:30:50.485350Z"},"trusted":true},"outputs":[],"source":["# save the model in its full form\n","model_emotional_full_path = \"/kaggle/working/robert-pretrained-emotional-full.pth\"\n","model_emotional.save_pretrained(model_emotional_full_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:20:33.664375Z","iopub.status.busy":"2024-05-14T16:20:33.663962Z","iopub.status.idle":"2024-05-14T16:20:34.090278Z","shell.execute_reply":"2024-05-14T16:20:34.089263Z","shell.execute_reply.started":"2024-05-14T16:20:33.664343Z"},"trusted":true},"outputs":[],"source":["# delete the emotional model from the GPU\n","del model_emotional\n","# reload the model\n","model_law = BertForMaskedLM.from_pretrained(\"readerbench/RoBERT-small\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:20:35.947279Z","iopub.status.busy":"2024-05-14T16:20:35.946782Z","iopub.status.idle":"2024-05-14T16:20:43.800466Z","shell.execute_reply":"2024-05-14T16:20:43.799535Z","shell.execute_reply.started":"2024-05-14T16:20:35.947228Z"},"trusted":true},"outputs":[],"source":["# load the romanian law dataset, that contains\n","# different legal texts\n","law_dataframe = pd.read_json(\"/kaggle/input/romanianlawforbert/legal.json\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:20:45.796519Z","iopub.status.busy":"2024-05-14T16:20:45.796149Z","iopub.status.idle":"2024-05-14T16:20:46.041891Z","shell.execute_reply":"2024-05-14T16:20:46.040729Z","shell.execute_reply.started":"2024-05-14T16:20:45.796494Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of texts extracted, of size 200: 6000 texts\n"]}],"source":["import re\n","\n","max_law_texts = 6000\n","max_law_text_size = 200\n","law_texts = []\n","\n","for text in law_dataframe.content:\n","    # stop condition for the whole extraction process, \n","    # because it does not need to extract more texts \n","    # if the maximum number of texts its reached\n","    if len(law_texts) >= max_law_texts:\n","        break\n","    \n","    text = re.sub(r\"nr\\s+\\d+\\s+din\\s+\\d+\\s+\\w+\\s+\\d+\", \"\", text)\n","    text = re.sub(r\"nr\\s+\\d+\", \"\", text)\n","    splitted_text = text.split()\n","    \n","    for index in range(0, len(splitted_text), max_law_text_size):\n","        # stop condition for the whole extraction process, \n","        # because it does not need to extract more texts \n","        # if the maximum number of texts its reached\n","        if len(law_texts) >= max_law_texts:\n","            break\n","        \n","        # the last text might contain less words, \n","        # so it is not needed\n","        if len(splitted_text[index:index + max_law_text_size]) < max_law_text_size:\n","            break\n","        \n","        mini_text = \" \".join(splitted_text[index:index + max_law_text_size])  \n","        law_texts.append(mini_text)\n","        \n","print(f\"Number of texts extracted, of size {max_law_text_size}: {len(law_texts)} texts\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:21:58.434279Z","iopub.status.busy":"2024-05-14T16:21:58.433630Z","iopub.status.idle":"2024-05-14T16:23:05.504373Z","shell.execute_reply":"2024-05-14T16:23:05.503302Z","shell.execute_reply.started":"2024-05-14T16:21:58.434245Z"},"trusted":true},"outputs":[],"source":["batch_size = 5\n","# move the MLM model to the device\n","model_law = model_law.to(device)\n","\n","# set the optimizer\n","optimizer = AdamW(model_law.parameters(), lr=1e-4)\n","\n","num_epochs = 4\n","\n","# total number of training steps\n","num_train_steps = len(train_dataloader) * num_epochs\n","\n","# scheduler (optional, for learning rate decay)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_train_steps\n",")\n","train_dataset = RomanianCorpusDataset(law_texts)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T16:23:46.845729Z","iopub.status.busy":"2024-05-14T16:23:46.845344Z","iopub.status.idle":"2024-05-14T16:50:37.516810Z","shell.execute_reply":"2024-05-14T16:50:37.515742Z","shell.execute_reply.started":"2024-05-14T16:23:46.845694Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:20,  5.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.12100644382337729\n","\n","Epoch 2/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.07582745673134923\n","\n","Epoch 3/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.06286146557889878\n","\n","Epoch 4/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.05289242000319064\n","\n","Epoch 5/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.04303169764423122\n","\n","Epoch 6/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.035808922922393925\n","\n","Epoch 7/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.029350762268683564\n","\n","Epoch 8/8\n"]},{"name":"stderr","output_type":"stream","text":["1200it [03:21,  5.96it/s]"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.024445067738803724\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["num_epochs = 8\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    loss = train(model_law, train_dataloader, optimizer, device, scheduler)\n","    print(f\"Loss: {loss}\")\n","    print()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T17:09:03.903091Z","iopub.status.busy":"2024-05-14T17:09:03.902631Z","iopub.status.idle":"2024-05-14T17:09:04.123254Z","shell.execute_reply":"2024-05-14T17:09:04.122137Z","shell.execute_reply.started":"2024-05-14T17:09:03.903059Z"},"trusted":true},"outputs":[],"source":["# save the model in its full form\n","model_law_dict_path = \"/kaggle/working/robert-pretrained-law\"\n","model_law.save_pretrained(model_law_dict_path)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5006827,"sourceId":8412205,"sourceType":"datasetVersion"},{"datasetId":5006836,"sourceId":8412216,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
